# Community paper analysis

This repository contains a Python module and Jupyter Notebooks with R workflows for processing and analyzing cell-cell communication data. The aim of this tool is to identify ligand-receptor interactions between cells and analyze potential differences in cell-cell communication in a diseased state, particularly in Acute Myeloid Leukemia (AML). The output of the analysis will include tables and figures.

## Aim / Overveiw

The objective of this repository is to reproduce the analysis of the expression in cell-cell communication in disease state and identify potential differences in cell-cell communication in the diseased state. By doing so, the `community` tool will provide insights into the mechanisms of cell-cell communication and its potential role in the development of AML.


## Workflow
We applied `community` tool on three published datasets.
    
1. Lasry, the dataset associated with this research has undergone peer review and has been published in the journal Nature Cancer. The publication can be accessed via the following link: https://doi.org/10.1038/s43018-022-00480-0. The raw dataset can be downloaded by running `download_Lasry_raw.sh`. 
2. Similie, a published scRNA dataset of 48 biopsies taken from the colon of 12 healthy and 18 ulcerative colitis(UC) individuals. This publication can be access via the following link: https://doi.org/10.1016/j.cell.2019.06.029. 
3. VanGalen_Hourigan, to study the alterations in cell type to cell type communication in AML, we created an integrated dataset containing the scRNAseq of the bone marrow of healthy individuals (GSE116256 vanGalenREF, GSE120221 OetjenREF) and AML patients at diagnosis(REF)

**_NOTE:_** If you want to replicate the results and skip the preprocessing step of each dataset, you can download the pre-processed datasets from the following Zenodo links. Alternatively, you can run the following command to download the datasets into their corresponding directories and proceed to run the notebooks.

The initial step of the analysis is conducted using the R module in the Jupyter Notebook `/src/data_preprocessing/$dataset/1.preprocess_data.ipynb`. This module handles tasks such as cleaning and processing the raw data, as well as annotating the data using relevant files.

Subsequently, the processed data from the first step is filtered through the Jupyter Notebook `/src/data_preprocessing/$dataset/2.filtering.ipynb`. This R module filters out cells with low library size and low gene count. The resulting processed data is stored in the `.RData` format within the `/results/data_preprocessing/$dataset/` directory.

Moving forward, the next stage, accomplished using the Jupyter Notebook `/src/data_preprocessing/$dataset/3.normalization.ipynb`, employs the Scran package in R for data normalization. The input for this stage is the filtered data obtained in the previous step, available in the `/results/data_preprocessing/$dataset/filtered/` directory.

The Jupyter Notebook `/src/data_preprocessing/$dataset/4.1.batch_correction.ipynb` utilizes the Python-based scgen library for batch correction. The input for this step is the normalized data from the previous stage.

**Warning:** Please be aware that running this notebook might take a substantial amount of time, depending on your computing system's specifications. It took nearly 14 hours to complete on a system with 128GB RAM and 30 CPUs. However, you can expedite the process by running it on a GPU node.

Lastly, the Jupyter Notebook `4.2.visualization.ipynb` is employed to visualize the processed data.


## Raw data info

Should we put some info about the raw data? 


## Supplementary tables

If need this section.


## Directory structure

The repository is organized into three main directories: `/src`, `/data`, and `/results`. The `/src` directory contains the Jupyter Notebooks that perform the analysis, `/data` directory contains the computed results from each analysis module, and `/results` directory is used to store the figures and tables generated by the respective modules.

The raw data should be downloaded into the /data/raw_data directory, which can be easily obtained by running the following command:

`make download-data`

## Resource usage

If this part is useful/neccessary, we need to check for this.

## How to run

- Clone the repo ```git clone https://github.com/colomemaria/community-paper.git``` and then cd into the directory ```cd community-paper/```

- If you don't have conda installed yet, install [conda](https://conda.io/miniconda.html) by running the command below

    ```
    make install-conda
    ```

- Create a conda environment named "community_paper" and install all necessary packages by using the following command:

    ```
    make create-env
    ```
- Launch [Jupyter](https://jupyter.org/) to access the notebooks to generate graphs

    ```
    make run-jupyter
    ```

- Go to [http://localhost:8888](http://localhost:8888) (a page should open automatically in your browser) 

    or
    
- Open:
    - [`src/1.preprocess_data.ipynb` Notebook](http://localhost:8888/notebooks/src/1.preprocess_data.ipynb) to run the demo workflow.
    
### Getting raw data

You can download the raw data to `/data/raw_data` folder by running the below command. You can also visit the link here XXX_zenodo_link and download manually. 

- Download raw data into` /data/raw_data` directory

    ```
    make download-data
    ```
